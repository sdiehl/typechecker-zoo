# Type Systems Overview

So why do we build type systems? The answer is obviously because they are awesome and intellectually interesting. But second to that, because they are genuinely useful.

Let's start with some background; you can technically skip this section and just dive into the code if you fancy, but it is helpful to set up some soft context before we dive into implementation.

Any discussion of type systems starts with the lambda calculus, a formal system developed by Alonzo Church in the 1930s to express computation. It is the minimal, universal programming language. Its syntax consists of just three elements: variables, function abstractions (a way to define an anonymous function), and function application (calling a function). For instance, the identity function, which simply returns its input, is written as \\( \lambda x. x \\). Despite this simplicity, any computable problem can be expressed and solved within the lambda calculus.

Next, we introduce type systems. A type system is a set of rules that assigns a property, known as a **type**, to the constructs of a program, such as variables, expressions, and functions. The primary purpose is to reduce bugs by preventing operations that don't make sense, like dividing a number by a string. This process of verifying that a program obeys its language's type rules is called type checking. 

## A System of Types

At the heart of any type system is a set of formal rules for making logical deductions about a program. These deductions are called **judgments**. A judgment is an assertion that a piece of code has a certain property. The most common kind of judgment you will encounter is a **typing judgment**, which asserts that a given expression has a particular type within a specific context. We write this formally using a "turnstile" symbol \\( \vdash \\). 

The general form of a typing judgment looks like this:

\\[ \Gamma \vdash e : T \\]

This statement is read as, "In the context \\( \Gamma \\), the expression \\( e \\) has the type \\( T \\)." The context \\( \Gamma \\) is essentially a map from variable names to their types. For example, \\( x: \text{Int}, f: \text{Bool} \to \text{Int} \\) is a context where the variable \\( x \\) has type \\( \text{Int} \\) and \\( f \\) has a function type from \\( \text{Bool} \\) to \\( \text{Int} \\). As we enter deeper scopes in a program, like inside a function body, we extend the context with new variable bindings. This is often written as \\( \Gamma, x:T \\), which means "the context \\( \Gamma \\) extended with a new binding stating that variable \\( x \\) has type \\( T \\)."

Typing judgments are derived using **inference rules**. An inference rule states that if you can prove a set of judgments, called the premises, then you can conclude another judgment, called the conclusion. When working with formal type systems, inference rules follow a consistent structural pattern that makes them easier to read and understand. Every rule has the form:

\\[ \frac{\text{premises}}{\text{conclusion}} \text{(Rule-Name)} \\]

This notation should be read as: "If all the premises above the line are true, then the conclusion below the line is also true." The premises represent the conditions that must be satisfied, while the conclusion represents what we can deduce when those conditions hold. The rule name provides a convenient label for referencing the rule in discussions and proofs.

If a rule has no premises, it is an **axiom**, a self-evident truth that requires no prior proof to hold. For example, an axiom rule that establishes the type of the literal zero might look like:

\\[ \frac{}{\Gamma \vdash 0 : \text{Int}} \text{(T-Zero)} \\]

This rule has no premises above the line, making it an axiom. It simply states that in any context \\( \Gamma \\), the literal \\( 0 \\) has type \\( \text{Int} \\).

These axioms typically handle simple cases like variable lookups or literal values (sometimes called **ground types**). Rules with multiple premises, separated by spacing or explicit conjunction symbols, require all conditions to be satisfied simultaneously before the conclusion can be drawn.

A foundational rule in nearly every type system is the variable lookup rule, which lets us find the type of a variable from the context:

\\[ \frac{x:T \in \Gamma}{\Gamma \vdash x : T} \\]

This rule is not an axiom because it has one premise. It reads: "If the type binding \\( x:T \\) is present in the context \\( \Gamma \\), then we can conclude that in context \\( \Gamma \\), the expression \\( x \\) has type \\( T \\)." It formally defines the action of looking up a variable's type in the current environment.

By defining a collection of these inference rules, we create a complete type system. Each rule defines how to determine the type of a specific kind of expression, like a function call, a literal value, or an if-then-else block. For instance, a rule for function application would require as its premises that we first prove the function itself has a function type \\( T \to U \\) and that its argument has the corresponding input type \\( T \\). If we can prove those premises, the rule allows us to conclude that the entire function application expression has the output type \\( U \\). By repeatedly applying these rules, we can build a derivation tree that starts from axioms about variables and literals and culminates in a single judgment about the type of our entire program, thereby proving it is well-typed.

## Judgements

A common inference rule with multiple premises is the one for function application, which determines the type of a function call. In lambda calculus, this is simply an expression \\( e_1 \ e_2 \\), where \\( e_1 \\) is the function and \\( e_2 \\) is the argument. To assign a type to this expression, we must first determine the types of both the function and its argument. The rule, often called "application" or "elimination for functions" (\\( \to E \\)), is written as follows:

\\[ \frac{\Gamma \vdash e_1 : T_1 \to T_2 \quad \Gamma \vdash e_2 : T_1}{\Gamma \vdash e_1 \ e_2 : T_2} \\]

This rule has two premises above the line. It states that to conclude that the application \\( e_1 \ e_2 \\) has type \\( T_2 \\), we must first prove two things in the same context \\( \Gamma \\). First, we must show that \\( e_1 \\) has a function type, written as \\( T_1 \to T_2 \\), which means it takes an input of type \\( T_1 \\) and produces an output of type \\( T_2 \\). Second, we must show that the argument \\( e_2 \\) has the correct input type \\( T_1 \\). If both of these premises hold, the rule allows us to deduce that the entire expression \\( e_1 \ e_2 \\) results in the function's output type, \\( T_2 \\).

Now, let's look at an example that chains three judgments together to type check the expression \\( f \ x \\). We'll work within a context \\( \Gamma \\) that contains bindings for both \\( f \\) and \\( x \\), specifically \\( \Gamma = f:\text{Int} \to \text{Bool}, x:\text{Int} \\). Our goal is to prove that \\( \Gamma \vdash f \ x : \text{Bool} \\).

Our derivation begins with two simple variable lookups, which are our axioms. These will form the premises of our application rule:

1.  **First Judgment (Variable Lookup for \\( f \\))**: We use the variable rule to find the type of \\( f \\). Since \\( f:\text{Int} \to \text{Bool} \\) is in our context \\( \Gamma \\), we can conclude:
    \\[ \frac{f:\text{Int} \to \text{Bool} \in \Gamma}{\Gamma \vdash f : \text{Int} \to \text{Bool}} \\]

2.  **Second Judgment (Variable Lookup for \\( x \\))**: Similarly, we look up the type of \\( x \\). The binding \\( x:\text{Int} \\) is in \\( \Gamma \\), so we can conclude:
    \\[ \frac{x:\text{Int} \in \Gamma}{\Gamma \vdash x : \text{Int}} \\]

3.  **Third Judgment (Function Application)**: Now we have the necessary premises to use the function application rule. We substitute \\( e_1 \\) with \\( f \\), \\( e_2 \\) with \\( x \\), \\( T_1 \\) with \\( \text{Int} \\), and \\( T_2 \\) with \\( \text{Bool} \\). Since our first two judgments successfully proved the premises, we can now form the final conclusion:
    \\[ \frac{\Gamma \vdash f : \text{Int} \to \text{Bool} \quad \Gamma \vdash x : \text{Int}}{\Gamma \vdash f \ x : \text{Bool}} \\]

Putting all of this together, we can represent the full derivation as a tree of nested inference rules, showing how the final judgment is built from the axioms for variable lookup:

\\[
\frac{
    \frac{f:\text{Int} \to \text{Bool} \in \Gamma}{\Gamma \vdash f : \text{Int} \to \text{Bool}}
    \quad
    \frac{x:\text{Int} \in \Gamma}{\Gamma \vdash x : \text{Int}}
}{
    \Gamma \vdash f \ x : \text{Bool}
}
\\]

This nested structure is called a **derivation tree** or **inference tree**. Each node in the tree corresponds to an application of an inference rule, and the leaves are axioms (variable lookups from the context). The tree visually demonstrates how the type checker starts from basic facts (the types of variables in the context) and applies rules step by step to reach a conclusion about the type of a complex expression. In this example, the root of the tree is the judgment \\( \Gamma \vdash f \ x : \text{Bool} \\), and its two children are the judgments for \\( f \\) and \\( x \\), each justified by their respective axioms. This process generalizes to larger programs, where the derivation tree grows to reflect the structure of the program and the logical flow of type information.

## Terms and Types

Historically, many programming languages enforced a strict separation between different layers of abstraction, a concept known as stratification. In this model, you had a "term language" and a "type language" which were syntactically and conceptually distinct. The term language consists of the expressions that actually compute values and run at runtime, like \\( 5 + 2 \\) or \\( \text{if } x \text{ then } y \text{ else } z \\). The type language, on the other hand, consists of expressions that describe the terms, like \\( \text{Int} \\) or \\( \text{Bool} \to \text{Bool} \\). These two worlds were kept separate; a type could not appear where a term was expected, and vice versa.

This stratification could be extended further. To bring order to the type language itself, a third layer called the "kind language" was often introduced. A kind can be thought of as the "type of a type." For example, a concrete type like \\( \text{Int} \\) has the simplest kind, \\( * \\) (often pronounced "type"). But a type constructor like \\( \text{List} \\) isn't a type on its own; it's something that takes a type and produces a new one. \\( \text{List} \\) takes \\( \text{Int} \\) to produce \\( \text{List} \ \text{Int} \\). Therefore, its kind is \\( * \to * \\). This creates a rigid hierarchy: terms are classified by types, and types are classified by kinds. This clear separation makes type checking simpler and more predictable.

However, a major trend in the design of modern, powerful type systems has been to move away from this strict stratification and instead unify the term and type languages into a single, consistent syntactic framework. In these unified systems, the line between what is a "value" (a term) and what is a "description" (a type) begins to blur. The language's grammar allows expressions that can be interpreted at different "levels" or "universes." For instance, you might have a universe \\( \text{Type}_0 \\) which contains simple types like \\( \text{Bool} \\). Then you would have a higher universe, \\( \text{Type}_1 \\), whose only member is \\( \text{Type}_0 \\). This allows you to write functions that operate on types themselves, a key feature of dependently typed languages. More on this later.

To make this concrete, let's consider a simple, stratified language for arithmetic. We can define its term language (\\( e \\)) and its corresponding type language (\\( \tau \\)) separately. The terms are the expressions we can compute, and the types are the static labels we can assign to them. Their definitions might look like this:

\\[
\begin{align*}
\text{terms} \quad e &::= n \mid e\_1 + e\_2 \mid \text{iszero}(e) \mid \text{true} \mid \text{false} \mid \text{if } e\_1 \text{ then } e\_2 \text{ else } e_3 \\\\
\text{types} \quad \tau &::= \text{Nat} \ \mid \ \text{Bool}
\end{align*}
\\]

Here, the term language \\( e \\) defines natural numbers (\\( n \\)), addition, a function to check for zero, boolean constants, and a conditional. The type language \\( \tau \\) is much simpler; it only contains the types \\( \text{Nat} \\) and \\( \text{Bool} \\). In this stratified system, an expression like \\( \text{Nat} + 5 \\) would be a syntax error because \\( \text{Nat} \\) belongs to the type language and cannot be used in a term-level operation like addition. In a more modern, unified system, this rigid distinction would be relaxed.

## Type Checking and Type Reconstruction

While the process of verifying that a program adheres to its type rules is called type checking, a related and historically significant challenge is **type reconstruction**, more commonly known as **type inference**. The goal of type inference is to have the compiler automatically deduce the types of expressions without the programmer needing to write explicit type annotations. For many years, developing algorithms that could perform full type inference for increasingly expressive languages was an active and vital area of research. The promise was seductive: achieve all the safety guarantees of a static type system without the verbose, manual effort of annotating every variable and function.

In more recent decades, the focus on achieving complete type inference has diminished. The primary reason is that as type systems have grown more powerful and complex, full inference has become computationally intractable or, in many cases, fundamentally undecidable. Modern languages often include features like higher-rank polymorphism (passing polymorphic functions as arguments), GADTs, and various forms of type-level programming where types themselves can involve computation. For these systems, a general algorithm that can always infer the single "best" type for any given expression simply does not exist. Attempting to do so would lead to impossibly complex algorithms and can result in inferred types that are enormous and incomprehensible to the programmer.

As a result, many modern statically-typed languages have converged on a practical and elegant middle ground: **bidirectional type checking**. Instead of having a single mode that always tries to infer types, a bidirectional checker operates in two distinct modes: a "checking" mode and a "synthesis" mode.

1.  **Checking Mode**: In this mode, the algorithm verifies that an expression \\( e \\) conforms to a known, expected type \\( \tau \\). Information flows "down" from the context into the expression. We ask the question: "Can we prove that \\( e \\) has type \\( \tau \\)?"
2.  **Synthesis Mode**: In this mode, the algorithm computes, or "synthesizes," a type for an expression \\( e \\) without any prior expectation. Information flows "up" from the expression's components. Here, we ask: "What is the type of \\( e \\)?"

This duality provides a powerful framework. The language designer can specify which syntactic constructs require annotations and which do not. For example, the arguments of a top-level function might require explicit annotations (putting the checker in checking mode for the function body), but the types of local variables within that body can be inferred (synthesized). This approach neatly sidesteps the difficulties of full inference by requiring the programmer to provide annotations only at key boundaries where ambiguity would otherwise arise. It offers a "best of both worlds" scenario: the convenience of local inference with the clarity and power of explicit annotations for complex, polymorphic, or ambiguous parts of the code, representing a theoretical sweet spot that balances expressiveness, usability, and implementability. More on this later.

## The Frontiers

A deep insight has been unfolding since the late 1970s, a revelation that three immense, seemingly distinct realms of human thought are echoes of a single, deeper reality. This ["computational trinitarianism"](https://ncatlab.org/nlab/show/computational+trilogy) reveals a profound interplay between different disciplines:

* Computation
* Formal logic
* Category theory

Think of it as a three way harmony between *Spaces*, *Logic* and *Computation*.

These are not merely analogous; they are three perspectives on one underlying phenomenon. The key to this unification is a powerful act of abstraction. While mathematics seems to be about many different things like sets and shapes, it fundamentally relies on the notion of a map. Set theory is about functions, topology is about continuous maps, algebra is about homomorphisms, and logic is about implication.

With the right framework, we can abstract away the specific details. What matters are not the objects themselves, but the maps between them. When you redefine a concept in this universal language, something incredible happens: you can translate it into any other category. A "group" defined in the category of sets is just a normal group. But translate that same definition to the category of topological spaces, and you get a topological group. In the category of manifolds, you get a Lie group. This is the Rosetta Stone for the foundations of abstraction itself.

This is why this is an exciting time in software. These concepts elevate types from simple data labels into a rich language for encoding deep truths about a program's behavior. These are no longer just academic curiosities; they are the architectural principles for the next generation of powerful and provably correct software.

This grand unification is an extension of the famous **Curry-Howard correspondence**, which revealed a stunning duality: a type is a logical proposition, and a program is a constructive proof of that proposition. Trinitarianism expands this into a three-fold revelation:

*   A **Proposition** in logic corresponds to a **Type** in a programming language, which corresponds to an **Object** (like a space) in a category.
*   A **Proof** of a proposition corresponds to a **Program** of a type, which corresponds to a **Map** (or morphism) in a category.
*   The **Simplification** of a proof corresponds to the **Computation** (or evaluation) of a program.

Under this paradigm, programming becomes the act of discovering and articulating mathematical truth. Consider a simple example: a function to get the first element of a list. A naive type signature might be `head(List<T>) -> T`. This is a weak proposition: "Give me a list of things, and I'll give you a thing." This proposition is false, as the program will crash on an empty list.

An expressive type system lets us state a more honest proposition: `safeHead(List<T>) -> Maybe<T>`. The `Maybe<T>` type embodies the possibility of failure. This type is a stronger proposition: "Given any list, I can provide a context that *maybe* contains a thing." A program satisfying this type is a *proof* that you have handled all possibilities, including the empty one. The compiler, by checking the type, verifies your proof and mathematically guarantees your program cannot crash from this error.

Now for a more profound example. Imagine a type for a *provably sorted* list: `SortedList<T>`. A function to merge two such lists has a type that is a powerful theorem: `merge(SortedList<T>, SortedList<T>) -> SortedList<T>`. This proposition states, "If you give me two proofs that lists are sorted, I will produce a new proof that the resulting merged list is also sorted." The compiler will not accept a program for this type unless its logic correctly preserves the "sorted" property. The type system becomes a partner in reasoning, ensuring deep logical properties hold true.

This idea of structure-preserving transformations is a core engine of modern mathematics. In algebraic topology, a functor acts as a map between entire universes of mathematical objects. The homology functor, for example, translates topological spaces into algebraic groups, allowing geometric problems to be solved with algebra. The insight of trinitarianism is that a well-typed program *is* a functor: a concrete, computable map that translates a logical proof into a computational process while perfectly preserving the underlying structure.

This theoretical bedrock unleashes extraordinary practical power. When types are precise logical statements, **composing programs** is like composing theorems, allowing us to build massive systems with certainty. The ultimate ambition this enables is **program synthesis**. The programmer's role shifts from mechanic to architect. They specify the *what* by writing a rich, descriptive type (a theorem), and the system discovers the *how* by searching for a valid proof, which is the program itself.

These foundations are likely to become critical as we enter an age of AI-generated code. As we increasingly task language models with generating vast software systems, the central challenge becomes one of trust. How do we know the code is correct? The answer is to demand more from the AI and from our type system that checks the code. We require it to produce not just code, but code whose very type is a formal proof of its correctness. The type checker then becomes our ultimate, verifiable arbiter of truth. And for that we will need richer type systems!

This transforms programming from a manual craft into a discipline of rigorous specification, allowing us to build software whose correctness is not just hoped for, but mathematically assured. This is the awesome power of modern type systems: the journey from preventing simple errors to proving deep truths, turning the art of programming into a direct collaboration with logic itself.

And that's why type systems are awesome, and hopefully you come to see how much fun they can be!
